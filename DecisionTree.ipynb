{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T15:00:41.441020",
     "start_time": "2017-12-07T15:00:41.191048"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisonTree(object):\n",
    "    def __init__(self,**kwargs):\n",
    "        '''\n",
    "        按照CART实现\n",
    "        task:Classification | Regression\n",
    "        or 后 都是 不传参数的默认值\n",
    "        '''\n",
    "        self.task = kwargs.get('task') or 'Classification'\n",
    "        self.max_depth =  kwargs.get('max_depth') or np.infty\n",
    "        min_samples_split = kwargs.get('min_samples_split')\n",
    "        self.min_samples_split = kwargs.get('min_samples_split') or 2\n",
    "        self.min_samples_leaf = kwargs.get('min_samples_leaf') or 1\n",
    "        self.min_impurity_decrease = kwargs.get('min_samples_leaf') or 0\n",
    "\n",
    "        \n",
    "    def train(self,X_train,y_train):\n",
    "        '''\n",
    "        对于类别属性来说，有两种处理方法：\n",
    "        1. 按属性中所有的值来分类来生成树（C3.0等早期分类树就是这种方式）\n",
    "        2. 将类别属性One-hot 编码来这样就只有0-1变量了。（Xgboost,CART 是这种)\n",
    "        3. 其实如果按照最好的离散属性分割方式，应该穷举所有的组合，找到一个最好的分割方式。H20好像有这种实践。\n",
    "\n",
    "        对于连续属性来说，就很好办了，直接二分分类。\n",
    "        属性中的值来说，一般来说，如果是（0,1）变量，那么每次分类后就可以去掉这个特征了，对于连续变量就不是了，要继续保留参与分割。\n",
    "\n",
    "        本算法采用方法2处理离散变量。即不再区分特征是离散变量还是连续变量，这样做能降低代码复杂度，而且做到了分类和回归任务的统一处理。\n",
    "        本算法仅仅生成二叉树，不生成多叉树。\n",
    "        '''\n",
    "        '''\n",
    "        不可采用分割方式，将子数据集传入递归函数，这样会丢失feature 位置，也就没法记录分割位置了，哈哈哈\n",
    "        算了，还是切割吧。简化程序处理\n",
    "        '''\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        tree = self.TreeGenerate(X_train,y_train)\n",
    "        self.Tree = self.pruning(tree)\n",
    "    def predict(self,X_test):\n",
    "        def get_node_value(X):\n",
    "            T = self.Tree\n",
    "            while True:\n",
    "                F = T['splitF']\n",
    "                V = T['splitV']\n",
    "                T = T['left'] if X[F] < V else T['right'] \n",
    "                if not isinstance(T,dict):\n",
    "                    return T\n",
    "        ret = np.apply_along_axis(get_node_value,1,X_test)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def TreeGenerate(self,X_train,y_train,depth = 0):\n",
    "        best_feature,best_value,best_error = self.chooseBestSplitfeature(X_train,y_train,depth)#选取最佳分裂属性，最佳分裂点\n",
    "        #print('best_feature,best_value::',best_feature,best_value)\n",
    "        if best_feature is None:#如果best_feature 为空，说明当前节点不满足分裂条件，函数直接返回节点值。\n",
    "            return best_value\n",
    "        Tree = {}\n",
    "        Tree['splitF'] = best_feature\n",
    "        Tree['splitV'] = best_value\n",
    "        Tree['best_error'] = best_error\n",
    "        Tree['depth'] = depth\n",
    "        left_X_train,left_y_train,right_X_train,right_y_train = self.binSplit(X_train,y_train,best_feature,best_value)\n",
    "        Tree['left'] = self.TreeGenerate(left_X_train,left_y_train,depth+1)\n",
    "        Tree['right'] = self.TreeGenerate(right_X_train,right_y_train,depth+1)\n",
    "        return Tree\n",
    "\n",
    "    def chooseBestSplitfeature(self,X_train,y_train,depth):\n",
    "        cls,cnt = np.unique(y_train,return_counts=True)#统计y值的数量\n",
    "        Error_ytrain = self.cal_Error(y_train)\n",
    "        NodeV_ytrain = self.get_NodeValue(y_train)\n",
    "        if (cls.shape[0]==1) or (depth > self.max_depth) or \\\n",
    "        ((X_train == X_train[0]).all()) or (X_train.shape[0] < self.min_samples_split):\n",
    "            #如果y值全部一样，停止分裂;超过最大高度，停止分裂;X中全部样本相同，停止分裂;X数量小于最小样本分割数，停止分裂\n",
    "            return None,NodeV_ytrain,Error_ytrain\n",
    "#         if depth > self.max_depth:#超过最大高度，停止分裂\n",
    "#             return None,get_NodeValue(y_train),Error_ytrain\n",
    "#         if (X_train == X_train[0]).all():\n",
    "#             #如果属性值为空为空，或者X中全部样本相同，则返回y中出现最多的值\n",
    "#             return None,get_NodeValue(y_train),Error_ytrain\n",
    "#         if X_train.shape[0] < self.min_samples_split:# 如果X数量小于最小样本分割数，停止分裂\n",
    "#             return None,get_NodeValue(y_train),Error_ytrain\n",
    "\n",
    "        current_error = Error_ytrain\n",
    "        #print('current_error:',current_error)\n",
    "        best_feature = 0\n",
    "        best_value = NodeV_ytrain\n",
    "        best_error = Error_ytrain\n",
    "        for i in range(X_train.shape[1]):\n",
    "            cls = np.unique(X_train[:,i])\n",
    "            if cls.shape[0]==1:\n",
    "                continue\n",
    "            cls = np.delete(cls,np.argmin(cls))#去掉一个最小值，开始枚举\n",
    "            for val in cls:\n",
    "                left_y_train,right_y_train = self.binSplit_y(X_train,y_train,i,val)\n",
    "                if (left_y_train.shape[0] < self.min_samples_leaf) or\\\n",
    "                (right_y_train.shape[0] <self.min_samples_leaf):#如果分裂后样本数量小于min_samples_leaf，停止分裂\n",
    "                    pass\n",
    "                else:#求较小的error\n",
    "                    error = self.cal_Error(left_y_train,right_y_train)\n",
    "                    if error < best_error:\n",
    "                        best_feature = i\n",
    "                        best_value = val\n",
    "                        best_error = error\n",
    "        if current_error - best_error <= self.min_impurity_decrease:\n",
    "            #Error减少过少，停止分裂\n",
    "            #print('case4 min_impurity_decrease',current_error - best_error)\n",
    "            return None,Error_ytrain,current_error\n",
    "\n",
    "        return best_feature,best_value,best_error\n",
    "\n",
    "    def binSplit(self,X_train,y_train,feature,value):\n",
    "        con_lt = np.where(X_train[:,feature]<value)\n",
    "        con_get = np.where(X_train[:,feature]>= value)\n",
    "        return X_train[con_lt],y_train[con_lt],X_train[con_get],y_train[con_get]            \n",
    "\n",
    "    def binSplit_y(self,X_train,y_train,feature,value):\n",
    "        #print(66,feature,value,end=';')\n",
    "        con_lt = np.where(X_train[:,feature]<value)\n",
    "        con_get = np.where(X_train[:,feature]>= value)\n",
    "        return y_train[con_lt],y_train[con_get]\n",
    "\n",
    "    def get_NodeValue(self,y_train):\n",
    "        '''\n",
    "        如果是分类任务，返回出现频率最高的一个值。\n",
    "        如果是回归任务，返回数据平均值\n",
    "        '''\n",
    "        if self.task == 'Classification':\n",
    "            cls,cnt = np.unique(y_train,return_counts=True)\n",
    "            return max(zip(cls,cnt),key=lambda x:x[1])[0]\n",
    "        elif self.task == 'Regression':\n",
    "            return np.mean(y_train)\n",
    "\n",
    "    def cal_Error(self,left_y_train,right_y_train=None):\n",
    "        '''\n",
    "\n",
    "        分类任务是 计算gini，熵,也可以用其他评估函数，比如交叉熵；回归任务一般是计算MAE。\n",
    "        refer:\n",
    "        [1] scikit-learn. http://scikit-learn.org/stable/modules/tree.html#classification-criteria\n",
    "        \n",
    "        '''\n",
    "        #np.mean(y_train)\n",
    "        if right_y_train is None:\n",
    "            return self.get_gini(left_y_train)\n",
    "        \n",
    "        left_gini = self.get_gini(left_y_train)\n",
    "        right_gini = self.get_gini(right_y_train)\n",
    "        gini = np.array([left_gini,right_gini])\n",
    "        shape_l = left_y_train.shape[0];shape_r = right_y_train.shape[0]\n",
    "        weight = np.array([shape_l,shape_r])/(shape_l+shape_r)\n",
    "        gini_index = (weight*gini).sum()\n",
    "        return gini_index\n",
    "\n",
    "    def get_gini(self,y_train):\n",
    "        '''\n",
    "        计算基尼系数，分类任务中会用到。\n",
    "        refer:\n",
    "        [1] 《机器学习》P79页4.2.3节基尼系数(周志华著，西瓜书)\n",
    "        '''\n",
    "        lens = y_train.shape[0]\n",
    "        vals,cnts = np.unique(y_train,return_counts=True)\n",
    "        #print(vals,cnts,cnts/lens)\n",
    "        gini = 1 - ((cnts/lens)**2).sum()# 就是这么简单...\n",
    "        return gini\n",
    "\n",
    "    def pruning(self,Tree):\n",
    "        '''\n",
    "        如果你把生成的树的图化出来，你会发现有些树的非叶子节点左右子树的值一样，这样肯定是不合理的，所以要有一个剪枝的buzhou\n",
    "        本函数只是为了优化树的结构，并不是用来防止过拟合的。\n",
    "        '''\n",
    "        if not isinstance(Tree,dict):\n",
    "            return Tree\n",
    "        if Tree['left'] == Tree['right']:\n",
    "            return Tree['left']\n",
    "        if isinstance(Tree['left'],dict):\n",
    "            Tree['left'] = self.pruning(Tree['left'])\n",
    "        if isinstance(Tree['right'],dict):\n",
    "            Tree['right'] = self.pruning(Tree['right'])\n",
    "        return Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T14:59:59.346586",
     "start_time": "2017-12-07T14:59:59.306413"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn import tree\n",
    "    iris = load_iris()\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "    DT = DecisonTree()\n",
    "    DT.train(X_train,y_train)\n",
    "    p = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一段利于调试编写的工具代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T14:57:08.785606",
     "start_time": "2017-12-07T14:57:08.779981"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    class self():\n",
    "        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "        max_depth = 5 \n",
    "        min_samples_split = 4\n",
    "        min_samples_leaf = 2\n",
    "        min_impurity_decrease = 0\n",
    "        max_depth = 3\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
